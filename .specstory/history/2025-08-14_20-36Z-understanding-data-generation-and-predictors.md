<!-- Generated by SpecStory -->

# Understanding data generation and predictors (2025-08-14 20:36Z)

_**User**_

How is the data generated and what are the predictors doing? Does it make sense to take these predictors

---

_**Assistant**_

Read file: undefined

---

<think><details><summary>Thought Process</summary>The data in the `source/Artificial_Data_alltogether.ipynb</details></think>

---

` notebook is generated as follows:

1.  **True Data Generation:**
    *   Job weights (`ws`) are created using a `bounded random walk` (defined in `random_walk_creation`). This means weights start randomly within a range `[m, M]` and then incrementally change by a `step_size`, staying within the bounds `m` and `M`.
    *   A `random_seed` is used for each experiment to ensure reproducibility.
    *   These weights, along with a deadline parameter `D`, are then used to create "true" job instances (`J_true`) using the `job_instance_creation` function. Each job has a weight, release time, and deadline.

2.  **Predictor Data Generation:**
    The notebook evaluates the Learning Augmented Scheduling (LAS) algorithm with three types of artificial predictors, each generating a set of "predicted" job weights (`w_pred`) which are then converted into predicted job instances (`J_pred`):
    *   **Accurate Predictor:** Generates `w_pred` by adding a small, random error (within `[-step_size, step_size]`) to the true weights (`w_true`). This simulates a good, but not perfect, prediction.
    *   **Random Predictor:** Generates `w_pred` as completely random values within `[m, M]`, independent of the true weights. This simulates a predictor that provides no useful information.
    *   **Misleading Predictor:** Generates `w_pred` by transforming `w_true` using the formula `(M - w_true) + m`. This creates a prediction that is inversely related to the true value, simulating an actively harmful prediction.

It makes sense to use these artificial predictors because they allow for:

*   **Controlled Experimentation:** By manipulating the quality of the predictions, the researchers can systematically study how the LAS algorithm performs under various scenarios (good prediction, no useful prediction, actively misleading prediction).
*   **Benchmarking:** Each predictor acts as a benchmark, helping to understand the algorithm's robustness and the impact of prediction quality on its efficiency (measured by energy ratio).
*   **Reproducibility:** The use of fixed random seeds ensures that the experiments are repeatable and verifiable.

---

